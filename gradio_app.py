import gradio as gr
import soundfile as sf
import tempfile
import torch
from vieneu_tts import VieNeuTTS
import os
import time
import numpy as np
import re
from typing import Generator
import queue
import threading
import yaml
from utils.core_utils import split_text_into_chunks

print("â³ Äang khá»Ÿi Ä‘á»™ng VieNeu-TTS...")

# --- CONSTANTS & CONFIG ---
CONFIG_PATH = os.path.join(os.path.dirname(__file__), "config.yaml")
try:
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        _config = yaml.safe_load(f) or {}
except Exception as e:
    raise RuntimeError(f"KhÃ´ng thá»ƒ Ä‘á»c config.yaml: {e}")

BACKBONE_CONFIGS = _config.get("backbone_configs", {})
CODEC_CONFIGS = _config.get("codec_configs", {})
VOICE_SAMPLES = _config.get("voice_samples", {})
_text_settings = _config.get("text_settings", {})
MAX_CHARS_PER_CHUNK = _text_settings.get("max_chars_per_chunk", 256)
MAX_TOTAL_CHARS_STREAMING = _text_settings.get("max_total_chars_streaming", 3000)

if not BACKBONE_CONFIGS or not CODEC_CONFIGS:
    raise ValueError("config.yaml thiáº¿u backbone_configs hoáº·c codec_configs")
if not VOICE_SAMPLES:
    raise ValueError("config.yaml thiáº¿u voice_samples")

# --- 1. MODEL CONFIGURATION ---

# Global model instance
tts = None
current_backbone = None
current_codec = None

def load_model(backbone_choice, codec_choice, device_choice):
    """Load model with specified configuration"""
    global tts, current_backbone, current_codec
    
    try:
        backbone_config = BACKBONE_CONFIGS[backbone_choice]
        codec_config = CODEC_CONFIGS[codec_choice]
        
        # Determine devices
        if device_choice == "Auto":
            if "GGUF" in backbone_choice:
                backbone_device = "gpu" if torch.cuda.is_available() else "cpu"
            else:
                backbone_device = "cuda" if torch.cuda.is_available() else "cpu"
            
            if "ONNX" in codec_choice:
                codec_device = "cpu"
            else:
                codec_device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            backbone_device = device_choice.lower()
            codec_device = device_choice.lower()
            
            if "ONNX" in codec_choice:
                codec_device = "cpu"
        
        if "GGUF" in backbone_choice and backbone_device == "cuda":
            backbone_device = "gpu"
        
        print(f"ğŸ“¦ Äang táº£i model...")
        print(f"   Backbone: {backbone_config['repo']} on {backbone_device}")
        print(f"   Codec: {codec_config['repo']} on {codec_device}")
        
        tts = VieNeuTTS(
            backbone_repo=backbone_config["repo"],
            backbone_device=backbone_device,
            codec_repo=codec_config["repo"],
            codec_device=codec_device
        )
        
        current_backbone = backbone_choice
        current_codec = codec_choice
        
        streaming_support = "âœ… CÃ³" if backbone_config['supports_streaming'] else "âŒ KhÃ´ng"
        preencoded_note = "\nâš ï¸ Codec nÃ y cáº§n sá»­ dá»¥ng pre-encoded codes (.pt files)" if codec_config['use_preencoded'] else ""
        
        return (
            f"âœ… Model Ä‘Ã£ táº£i thÃ nh cÃ´ng!\n\n"
            f"ğŸ¦œ Model Device: {backbone_device.upper()}\n\n"
            f"ğŸµ Codec Device: {codec_device.upper()}{preencoded_note}"
        )
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return f"âŒ Lá»—i khi táº£i model: {str(e)}"

# --- 2. DATA & HELPERS ---

GGUF_ALLOWED_VOICES = [
    "VÄ©nh (nam miá»n Nam)",
    "BÃ¬nh (nam miá»n Báº¯c)",
    "Ngá»c (ná»¯ miá»n Báº¯c)",
    "Dung (ná»¯ miá»n Nam)",
]


def get_voice_options(backbone_choice: str):
    """Filter voice options: GGUF only shows the 4 allowed voices."""
    if "GGUF" in backbone_choice:
        return [v for v in GGUF_ALLOWED_VOICES if v in VOICE_SAMPLES]
    return list(VOICE_SAMPLES.keys())


def update_voice_dropdown(backbone_choice: str, current_voice: str):
    options = get_voice_options(backbone_choice)
    new_value = current_voice if current_voice in options else (options[0] if options else None)
    # gr.update is available across Gradio versions to update component props
    return gr.update(choices=options, value=new_value)


# --- 3. CORE LOGIC FUNCTIONS ---

def load_reference_info(voice_choice):
    if voice_choice in VOICE_SAMPLES:
        audio_path = VOICE_SAMPLES[voice_choice]["audio"]
        text_path = VOICE_SAMPLES[voice_choice]["text"]
        try:
            if os.path.exists(text_path):
                with open(text_path, "r", encoding="utf-8") as f:
                    ref_text = f.read()
                return audio_path, ref_text
            else:
                return audio_path, "âš ï¸ KhÃ´ng tÃ¬m tháº¥y file text máº«u."
        except Exception as e:
            return None, f"âŒ Lá»—i: {str(e)}"
    return None, ""

def synthesize_speech(text, voice_choice, custom_audio, custom_text, mode_tab, generation_mode):
    """Cáº£i tiáº¿n streaming vá»›i pre-buffering vÃ  crossfade mÆ°á»£t hÆ¡n"""
    global tts, current_backbone, current_codec
    
    # === VALIDATION (giá»¯ nguyÃªn) ===
    if tts is None:
        yield None, "âš ï¸ Vui lÃ²ng táº£i model trÆ°á»›c!"
        return
    if not text or text.strip() == "":
        yield None, "âš ï¸ Vui lÃ²ng nháº­p vÄƒn báº£n!"
        return

    raw_text = text.strip()
    codec_config = CODEC_CONFIGS[current_codec]
    use_preencoded = codec_config['use_preencoded']

    # Setup Reference (giá»¯ nguyÃªn logic cÅ©)
    if mode_tab == "custom_mode": 
        if custom_audio is None or not custom_text:
            yield None, "âš ï¸ Thiáº¿u Audio hoáº·c Text máº«u custom."
            return
        ref_audio_path = custom_audio
        ref_text_raw = custom_text
        ref_codes_path = None
    else:
        if voice_choice not in VOICE_SAMPLES:
            yield None, "âš ï¸ Vui lÃ²ng chá»n giá»ng máº«u."
            return
        ref_audio_path = VOICE_SAMPLES[voice_choice]["audio"]
        ref_text_path = VOICE_SAMPLES[voice_choice]["text"]
        ref_codes_path = VOICE_SAMPLES[voice_choice]["codes"]
        if not os.path.exists(ref_audio_path):
            yield None, "âŒ KhÃ´ng tÃ¬m tháº¥y file audio máº«u."
            return
        with open(ref_text_path, "r", encoding="utf-8") as f:
            ref_text_raw = f.read()

    yield None, "ğŸ“„ Äang xá»­ lÃ½ Reference..."
    
    # Encode reference
    try:
        if use_preencoded and ref_codes_path and os.path.exists(ref_codes_path):
            ref_codes = torch.load(ref_codes_path, map_location="cpu")
        else:
            ref_codes = tts.encode_reference(ref_audio_path)
        if isinstance(ref_codes, torch.Tensor):
            ref_codes = ref_codes.cpu().numpy()
    except Exception as e:
        yield None, f"âŒ Lá»—i xá»­ lÃ½ reference: {e}"
        return

    text_chunks = split_text_into_chunks(raw_text, max_chars=MAX_CHARS_PER_CHUNK)
    total_chunks = len(text_chunks)

    # === STANDARD MODE ===
    if generation_mode == "Standard (Má»™t láº§n)":
        yield None, f"ğŸš€ Báº¯t Ä‘áº§u tá»•ng há»£p cháº¿ Ä‘á»™ Standard ({total_chunks} Ä‘oáº¡n)..."
        all_audio_segments = []
        sr = 24000
        silence_pad = np.zeros(int(sr * 0.15), dtype=np.float32)
        start_time = time.time()
        
        try:
            for i, chunk in enumerate(text_chunks):
                yield None, f"â³ Äang xá»­ lÃ½ Ä‘oáº¡n {i+1}/{total_chunks}..."
                chunk_wav = tts.infer(chunk, ref_codes, ref_text_raw)
                if chunk_wav is not None and len(chunk_wav) > 0:
                    all_audio_segments.append(chunk_wav)
                    if i < total_chunks - 1:
                        all_audio_segments.append(silence_pad)
            
            if not all_audio_segments:
                yield None, "âŒ KhÃ´ng sinh Ä‘Æ°á»£c audio nÃ o."
                return

            yield None, "ğŸ’¾ Äang ghÃ©p file vÃ  lÆ°u..."
            final_wav = np.concatenate(all_audio_segments)
            
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                sf.write(tmp.name, final_wav, sr)
                output_path = tmp.name
            
            process_time = time.time() - start_time
            yield output_path, f"âœ… HoÃ n táº¥t! (Tá»•ng thá»i gian: {process_time:.2f}s)"

        except Exception as e:
            import traceback
            traceback.print_exc()
            yield None, f"âŒ Lá»—i Standard Mode: {str(e)}"
        return

    # === STREAMING MODE ===
    else:
        sr = 24000
        crossfade_samples = int(sr * 0.03)
        
        # Cáº¢I TIáº¾N 1: TÄƒng buffer size vÃ  thÃªm pre-buffering
        audio_queue = queue.Queue(maxsize=100)
        PRE_BUFFER_SIZE = 3  # Chá» 3 chunks trÆ°á»›c khi báº¯t Ä‘áº§u phÃ¡t
        
        end_event = threading.Event()
        error_event = threading.Event()
        error_msg = ""

        def producer_thread():
            nonlocal error_msg
            try:
                previous_tail = None
                chunk_count = 0
                
                for i, chunk_text in enumerate(text_chunks):
                    stream_gen = tts.infer_stream(chunk_text, ref_codes, ref_text_raw)
                    
                    for part_idx, audio_part in enumerate(stream_gen):
                        if audio_part is None or len(audio_part) == 0:
                            continue
                        
                        if previous_tail is not None and len(previous_tail) > 0:
                            overlap = min(len(previous_tail), len(audio_part), crossfade_samples)
                            if overlap > 0:
                                fade_out = np.linspace(1.0, 0.0, overlap, dtype=np.float32)
                                fade_in = np.linspace(0.0, 1.0, overlap, dtype=np.float32)
                                
                                # Káº¿t há»£p pháº§n overlap
                                blended = (audio_part[:overlap] * fade_in + 
                                          previous_tail[-overlap:] * fade_out)
                                
                                processed = np.concatenate([
                                    previous_tail[:-overlap] if len(previous_tail) > overlap else np.array([]),
                                    blended,
                                    audio_part[overlap:]
                                ])
                            else:
                                processed = np.concatenate([previous_tail, audio_part])
                            
                            tail_size = min(crossfade_samples, len(processed))
                            previous_tail = processed[-tail_size:].copy()
                            output_chunk = processed[:-tail_size] if len(processed) > tail_size else processed
                        else:
                            tail_size = min(crossfade_samples, len(audio_part))
                            previous_tail = audio_part[-tail_size:].copy()
                            output_chunk = audio_part[:-tail_size] if len(audio_part) > tail_size else audio_part
                        
                        if len(output_chunk) > 0:
                            audio_queue.put((sr, output_chunk))
                            chunk_count += 1
                
                if previous_tail is not None and len(previous_tail) > 0:
                    audio_queue.put((sr, previous_tail))
                    
            except Exception as e:
                import traceback
                traceback.print_exc()
                error_msg = str(e)
                error_event.set()
            finally:
                end_event.set()
                audio_queue.put(None)

        threading.Thread(target=producer_thread, daemon=True).start()
        
        yield (sr, np.zeros(int(sr * 0.05))), "ğŸ”„ Äang buffering..."
        
        pre_buffer = []
        while len(pre_buffer) < PRE_BUFFER_SIZE:
            try:
                item = audio_queue.get(timeout=5.0)
                if item is None:
                    break
                pre_buffer.append(item)
            except queue.Empty:
                if error_event.is_set():
                    yield None, f"âŒ Lá»—i: {error_msg}"
                    return
                break
        
        # Báº¯t Ä‘áº§u phÃ¡t pre-buffer
        full_audio_buffer = []
        for sr, audio_data in pre_buffer:
            full_audio_buffer.append(audio_data)
            yield (sr, audio_data), "ğŸ”Š Äang phÃ¡t..."
        
        # Tiáº¿p tá»¥c phÃ¡t pháº§n cÃ²n láº¡i
        while True:
            try:
                item = audio_queue.get(timeout=0.05)
                if item is None:
                    break
                sr, audio_data = item
                full_audio_buffer.append(audio_data)
                yield (sr, audio_data), "ğŸ”Š Äang phÃ¡t..."
            except queue.Empty:
                if error_event.is_set():
                    yield None, f"âŒ Lá»—i: {error_msg}"
                    break
                if end_event.is_set() and audio_queue.empty():
                    break
                continue

        # LÆ°u file hoÃ n chá»‰nh
        if full_audio_buffer:
            final_wav = np.concatenate(full_audio_buffer)
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                sf.write(tmp.name, final_wav, sr)
            yield tmp.name, "âœ… HoÃ n táº¥t Streaming!"

# --- 4. UI SETUP ---
theme = gr.themes.Ocean(
    primary_hue="indigo",
    secondary_hue="cyan",
    neutral_hue="slate",
    font=[gr.themes.GoogleFont('Inter'), 'ui-sans-serif', 'system-ui'],
).set(
    button_primary_background_fill="linear-gradient(90deg, #6366f1 0%, #0ea5e9 100%)",
    button_primary_background_fill_hover="linear-gradient(90deg, #4f46e5 0%, #0284c7 100%)",
)

css = """
.container { max-width: 1400px; margin: auto; }
.header-box { text-align: center; margin-bottom: 25px; padding: 25px; background: linear-gradient(135deg, #0f172a 0%, #1e293b 100%); border-radius: 12px; color: white; }
.header-title { font-size: 2.5rem; font-weight: 800; background: -webkit-linear-gradient(45deg, #60A5FA, #22D3EE); -webkit-background-clip: text; -webkit-text-fill-color: transparent; }
.status-box { font-weight: bold; text-align: center; border: none; background: transparent; }
"""

EXAMPLES_LIST = [
    ["Vá» miá»n TÃ¢y khÃ´ng chá»‰ Ä‘á»ƒ ngáº¯m nhÃ¬n sÃ´ng nÆ°á»›c há»¯u tÃ¬nh, mÃ  cÃ²n Ä‘á»ƒ cáº£m nháº­n táº¥m chÃ¢n tÃ¬nh cá»§a ngÆ°á»i dÃ¢n nÆ¡i Ä‘Ã¢y.", "VÄ©nh (nam miá»n Nam)"],
    ["HÃ  Ná»™i nhá»¯ng ngÃ y vÃ o thu mang má»™t váº» Ä‘áº¹p tráº§m máº·c vÃ  cá»• kÃ­nh Ä‘áº¿n láº¡ thÆ°á»ng.", "BÃ¬nh (nam miá»n Báº¯c)"],
]

with gr.Blocks(theme=theme, css=css, title="VieNeu-TTS") as demo:
    
    with gr.Column(elem_classes="container"):
        gr.HTML("""<div class="header-box"><div class="header-title">ğŸ¦œ VieNeu-TTS Studio</div></div>""")
        
        # --- CONFIGURATION ---
        with gr.Group():
            with gr.Row():
                backbone_select = gr.Dropdown(list(BACKBONE_CONFIGS.keys()), value="GGUF Q8", label="ğŸ¦œ Backbone")
                codec_select = gr.Dropdown(list(CODEC_CONFIGS.keys()), value="NeuCodec (Standard)", label="ğŸµ Codec")
                device_choice = gr.Radio(["Auto", "CPU", "CUDA"], value="Auto", label="ğŸ–¥ï¸ Device")
            
            with gr.Row():
                btn_load = gr.Button("ğŸ”„ Táº£i Model", variant="primary")
            model_status = gr.Markdown("â³ ChÆ°a táº£i model.")
    
    with gr.Row(elem_classes="container"):
        # --- INPUT ---
        with gr.Column(scale=3):
            text_input = gr.Textbox(
                label=f"VÄƒn báº£n (Streaming há»— trá»£ tá»›i {MAX_TOTAL_CHARS_STREAMING} kÃ½ tá»±, chia chunk {MAX_CHARS_PER_CHUNK} kÃ½ tá»±)", 
                lines=4, 
                value="HÃ  Ná»™i, trÃ¡i tim cá»§a Viá»‡t Nam, lÃ  má»™t thÃ nh phá»‘ ngÃ n nÄƒm vÄƒn hiáº¿n vá»›i bá» dÃ y lá»‹ch sá»­ vÃ  vÄƒn hÃ³a Ä‘á»™c Ä‘Ã¡o. BÆ°á»›c chÃ¢n trÃªn nhá»¯ng con phá»‘ cá»• kÃ­nh quanh Há»“ HoÃ n Kiáº¿m, du khÃ¡ch nhÆ° Ä‘Æ°á»£c du hÃ nh ngÆ°á»£c thá»i gian, chiÃªm ngÆ°á»¡ng kiáº¿n trÃºc PhÃ¡p cá»• Ä‘iá»ƒn hÃ²a quyá»‡n vá»›i nÃ©t kiáº¿n trÃºc truyá»n thá»‘ng Viá»‡t Nam. Má»—i con phá»‘ trong khu phá»‘ cá»• mang má»™t tÃªn gá»i Ä‘áº·c trÆ°ng, pháº£n Ã¡nh nghá» thá»§ cÃ´ng truyá»n thá»‘ng tá»«ng thá»‹nh hÃ nh nÆ¡i Ä‘Ã¢y nhÆ° phá»‘ HÃ ng Báº¡c, HÃ ng ÄÃ o, HÃ ng MÃ£. áº¨m thá»±c HÃ  Ná»™i cÅ©ng lÃ  má»™t Ä‘iá»ƒm nháº¥n Ä‘áº·c biá»‡t, tá»« tÃ´ phá»Ÿ nÃ³ng há»•i buá»•i sÃ¡ng, bÃºn cháº£ thÆ¡m lá»«ng trÆ°a hÃ¨, Ä‘áº¿n chÃ¨ ThÃ¡i ngá»t ngÃ o chiá»u thu. Nhá»¯ng mÃ³n Äƒn dÃ¢n dÃ£ nÃ y Ä‘Ã£ trá»Ÿ thÃ nh biá»ƒu tÆ°á»£ng cá»§a vÄƒn hÃ³a áº©m thá»±c Viá»‡t, Ä‘Æ°á»£c cáº£ tháº¿ giá»›i yÃªu máº¿n. NgÆ°á»i HÃ  Ná»™i ná»•i tiáº¿ng vá»›i tÃ­nh cÃ¡ch hiá»n hÃ²a, lá»‹ch thiá»‡p nhÆ°ng cÅ©ng ráº¥t cáº§u toÃ n trong tá»«ng chi tiáº¿t nhá», tá»« cÃ¡ch pha trÃ  sen cho Ä‘áº¿n cÃ¡ch chá»n hoa sen tÃ¢y Ä‘á»ƒ thÆ°á»Ÿng trÃ .",
            )
            
            with gr.Tabs() as tabs:
                with gr.TabItem("ğŸ‘¤ Preset", id="preset_mode"):
                    initial_voices = get_voice_options("GGUF Q8")
                    default_voice = initial_voices[0] if initial_voices else None
                    voice_select = gr.Dropdown(initial_voices, value=default_voice, label="Giá»ng máº«u")
                with gr.TabItem("ğŸ™ï¸ Custom", id="custom_mode"):
                    custom_audio = gr.Audio(label="File máº«u (.wav)", type="filepath")
                    custom_text = gr.Textbox(label="Lá»i thoáº¡i máº«u")

            generation_mode = gr.Radio(
                ["Standard (Má»™t láº§n)", "Streaming (Tá»«ng Ä‘oáº¡n)"], 
                value="Streaming (Tá»«ng Ä‘oáº¡n)", 
                label="Cháº¿ Ä‘á»™ sinh"
            )
            current_mode = gr.Textbox(visible=False, value="preset_mode")
            btn_generate = gr.Button("ğŸµ Báº¯t Ä‘áº§u", variant="primary", size="lg")

        # --- OUTPUT ---
        with gr.Column(scale=2):
            audio_output = gr.Audio(
                label="Káº¿t quáº£", 
                type="filepath", 
                autoplay=True,
                show_download_button=True
            )
            status_output = gr.Textbox(label="Tráº¡ng thÃ¡i", elem_classes="status-box")

    # --- EVENT HANDLERS ---
    
    def update_info(backbone):
        return f"Streaming: {'âœ…' if BACKBONE_CONFIGS[backbone]['supports_streaming'] else 'âŒ'}"
    backbone_select.change(update_info, backbone_select, model_status)
    backbone_select.change(update_voice_dropdown, [backbone_select, voice_select], voice_select)

    tabs.children[0].select(lambda: "preset_mode", outputs=current_mode)
    tabs.children[1].select(lambda: "custom_mode", outputs=current_mode)

    btn_load.click(load_model, [backbone_select, codec_select, device_choice], model_status)

    btn_generate.click(
        fn=synthesize_speech,
        inputs=[text_input, voice_select, custom_audio, custom_text, current_mode, generation_mode],
        outputs=[audio_output, status_output]
    )

if __name__ == "__main__":
    demo.queue().launch(server_name="127.0.0.1", server_port=7860)