import gradio as gr
import soundfile as sf
import tempfile
import torch
from vieneu_tts import VieNeuTTS
import os
import time
import numpy as np
import re
from typing import Generator
import queue
import threading
import yaml
from utils.core_utils import split_text_into_chunks

print("‚è≥ ƒêang kh·ªüi ƒë·ªông VieNeu-TTS...")

# --- CONSTANTS & CONFIG ---
CONFIG_PATH = os.path.join(os.path.dirname(__file__), "config.yaml")
try:
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        _config = yaml.safe_load(f) or {}
except Exception as e:
    raise RuntimeError(f"Kh√¥ng th·ªÉ ƒë·ªçc config.yaml: {e}")

BACKBONE_CONFIGS = _config.get("backbone_configs", {})
CODEC_CONFIGS = _config.get("codec_configs", {})
VOICE_SAMPLES = _config.get("voice_samples", {})

_text_settings = _config.get("text_settings", {})
MAX_CHARS_PER_CHUNK = _text_settings.get("max_chars_per_chunk", 256)
MAX_TOTAL_CHARS_STREAMING = _text_settings.get("max_total_chars_streaming", 3000)

if not BACKBONE_CONFIGS or not CODEC_CONFIGS:
    raise ValueError("config.yaml thi·∫øu backbone_configs ho·∫∑c codec_configs")
if not VOICE_SAMPLES:
    raise ValueError("config.yaml thi·∫øu voice_samples")

# --- 1. MODEL CONFIGURATION ---
# Global model instance
tts = None
current_backbone = None
current_codec = None
model_loaded = False  # ‚ú® TH√äM STATE

def load_model(backbone_choice, codec_choice, device_choice):
    """Load model with specified configuration"""
    global tts, current_backbone, current_codec, model_loaded
    
    # ‚ú® Tr·∫£ v·ªÅ nhi·ªÅu outputs ƒë·ªÉ update UI ngay l·∫≠p t·ª©c
    yield (
        "‚è≥ ƒêang t·∫£i model, vui l√≤ng ƒë·ª£i...",
        gr.update(interactive=False),  # Disable n√∫t "B·∫Øt ƒë·∫ßu"
        gr.update(interactive=False)   # Disable n√∫t "T·∫£i Model"
    )
    
    try:
        backbone_config = BACKBONE_CONFIGS[backbone_choice]
        codec_config = CODEC_CONFIGS[codec_choice]
        
        # Determine devices
        if device_choice == "Auto":
            if "GGUF" in backbone_choice:
                backbone_device = "gpu" if torch.cuda.is_available() else "cpu"
            else:
                backbone_device = "cuda" if torch.cuda.is_available() else "cpu"
            
            if "ONNX" in codec_choice:
                codec_device = "cpu"
            else:
                codec_device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            backbone_device = device_choice.lower()
            codec_device = device_choice.lower()
            
            if "ONNX" in codec_choice:
                codec_device = "cpu"
        
        if "GGUF" in backbone_choice and backbone_device == "cuda":
            backbone_device = "gpu"
        
        print(f"üì¶ ƒêang t·∫£i model...")
        print(f"   Backbone: {backbone_config['repo']} on {backbone_device}")
        print(f"   Codec: {codec_config['repo']} on {codec_device}")
        
        tts = VieNeuTTS(
            backbone_repo=backbone_config["repo"],
            backbone_device=backbone_device,
            codec_repo=codec_config["repo"],
            codec_device=codec_device
        )
        
        current_backbone = backbone_choice
        current_codec = codec_choice
        model_loaded = True  # ‚ú® ƒê√°nh d·∫•u ƒë√£ load xong
        
        streaming_support = "‚úÖ C√≥" if backbone_config['supports_streaming'] else "‚ùå Kh√¥ng"
        preencoded_note = "\n‚ö†Ô∏è Codec n√†y c·∫ßn s·ª≠ d·ª•ng pre-encoded codes (.pt files)" if codec_config['use_preencoded'] else ""
        
        success_msg = (
            f"‚úÖ Model ƒë√£ t·∫£i th√†nh c√¥ng!\n\n"
            f"ü¶ú Model Device: {backbone_device.upper()}\n\n"
            f"üéµ Codec Device: {codec_device.upper()}{preencoded_note}"
        )
        
        yield (
            success_msg,
            gr.update(interactive=True),   # ‚ú® Enable n√∫t "B·∫Øt ƒë·∫ßu"
            gr.update(interactive=True)    # Enable n√∫t "T·∫£i Model"
        )
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        model_loaded = False
        
        yield (
            f"‚ùå L·ªói khi t·∫£i model: {str(e)}",
            gr.update(interactive=False),  # V·∫´n disable n√∫t "B·∫Øt ƒë·∫ßu"
            gr.update(interactive=True)    # Enable n√∫t "T·∫£i Model" ƒë·ªÉ th·ª≠ l·∫°i
        )

# --- 2. DATA & HELPERS ---
GGUF_ALLOWED_VOICES = [
    "Vƒ©nh (nam mi·ªÅn Nam)",
    "B√¨nh (nam mi·ªÅn B·∫Øc)",
    "Ng·ªçc (n·ªØ mi·ªÅn B·∫Øc)",
    "Dung (n·ªØ mi·ªÅn Nam)",
]

def get_voice_options(backbone_choice: str):
    """Filter voice options: GGUF only shows the 4 allowed voices."""
    if "GGUF" in backbone_choice:
        return [v for v in GGUF_ALLOWED_VOICES if v in VOICE_SAMPLES]
    return list(VOICE_SAMPLES.keys())

def update_voice_dropdown(backbone_choice: str, current_voice: str):
    options = get_voice_options(backbone_choice)
    new_value = current_voice if current_voice in options else (options[0] if options else None)
    return gr.update(choices=options, value=new_value)

# --- 3. CORE LOGIC FUNCTIONS ---
def load_reference_info(voice_choice):
    if voice_choice in VOICE_SAMPLES:
        audio_path = VOICE_SAMPLES[voice_choice]["audio"]
        text_path = VOICE_SAMPLES[voice_choice]["text"]
        try:
            if os.path.exists(text_path):
                with open(text_path, "r", encoding="utf-8") as f:
                    ref_text = f.read()
                return audio_path, ref_text
            else:
                return audio_path, "‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file text m·∫´u."
        except Exception as e:
            return None, f"‚ùå L·ªói: {str(e)}"
    return None, ""

def synthesize_speech(text, voice_choice, custom_audio, custom_text, mode_tab, generation_mode):
    """Synthesis with model check"""
    global tts, current_backbone, current_codec, model_loaded
    
    # ‚ú® Ki·ªÉm tra model ƒë√£ load ch∆∞a
    if not model_loaded or tts is None:
        yield None, "‚ö†Ô∏è Vui l√≤ng t·∫£i model tr∆∞·ªõc!"
        return
    
    if not text or text.strip() == "":
        yield None, "‚ö†Ô∏è Vui l√≤ng nh·∫≠p vƒÉn b·∫£n!"
        return
    
    raw_text = text.strip()
    
    codec_config = CODEC_CONFIGS[current_codec]
    use_preencoded = codec_config['use_preencoded']
    
    # Setup Reference
    if mode_tab == "custom_mode":
        if custom_audio is None or not custom_text:
            yield None, "‚ö†Ô∏è Thi·∫øu Audio ho·∫∑c Text m·∫´u custom."
            return
        ref_audio_path = custom_audio
        ref_text_raw = custom_text
        ref_codes_path = None
    else:
        if voice_choice not in VOICE_SAMPLES:
            yield None, "‚ö†Ô∏è Vui l√≤ng ch·ªçn gi·ªçng m·∫´u."
            return
        ref_audio_path = VOICE_SAMPLES[voice_choice]["audio"]
        ref_text_path = VOICE_SAMPLES[voice_choice]["text"]
        ref_codes_path = VOICE_SAMPLES[voice_choice]["codes"]
        
        if not os.path.exists(ref_audio_path):
            yield None, "‚ùå Kh√¥ng t√¨m th·∫•y file audio m·∫´u."
            return
        
        with open(ref_text_path, "r", encoding="utf-8") as f:
            ref_text_raw = f.read()
    
    yield None, "üìÑ ƒêang x·ª≠ l√Ω Reference..."
    
    # Encode reference
    try:
        if use_preencoded and ref_codes_path and os.path.exists(ref_codes_path):
            ref_codes = torch.load(ref_codes_path, map_location="cpu")
        else:
            ref_codes = tts.encode_reference(ref_audio_path)
        
        if isinstance(ref_codes, torch.Tensor):
            ref_codes = ref_codes.cpu().numpy()
    except Exception as e:
        yield None, f"‚ùå L·ªói x·ª≠ l√Ω reference: {e}"
        return
    
    text_chunks = split_text_into_chunks(raw_text, max_chars=MAX_CHARS_PER_CHUNK)
    total_chunks = len(text_chunks)
    
    # === STANDARD MODE ===
    if generation_mode == "Standard (M·ªôt l·∫ßn)":
        yield None, f"üöÄ B·∫Øt ƒë·∫ßu t·ªïng h·ª£p ch·∫ø ƒë·ªô Standard ({total_chunks} ƒëo·∫°n)..."
        
        all_audio_segments = []
        sr = 24000
        silence_pad = np.zeros(int(sr * 0.15), dtype=np.float32)
        
        start_time = time.time()
        
        try:
            for i, chunk in enumerate(text_chunks):
                yield None, f"‚è≥ ƒêang x·ª≠ l√Ω ƒëo·∫°n {i+1}/{total_chunks}..."
                
                chunk_wav = tts.infer(chunk, ref_codes, ref_text_raw)
                
                if chunk_wav is not None and len(chunk_wav) > 0:
                    all_audio_segments.append(chunk_wav)
                    if i < total_chunks - 1:
                        all_audio_segments.append(silence_pad)
            
            if not all_audio_segments:
                yield None, "‚ùå Kh√¥ng sinh ƒë∆∞·ª£c audio n√†o."
                return
            
            yield None, "üíæ ƒêang gh√©p file v√† l∆∞u..."
            
            final_wav = np.concatenate(all_audio_segments)
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                sf.write(tmp.name, final_wav, sr)
                output_path = tmp.name
            
            process_time = time.time() - start_time
            yield output_path, f"‚úÖ Ho√†n t·∫•t! (T·ªïng th·ªùi gian: {process_time:.2f}s)"
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            yield None, f"‚ùå L·ªói Standard Mode: {str(e)}"
            return
    
    # === STREAMING MODE ===
    else:
        sr = 24000
        crossfade_samples = int(sr * 0.03)
        audio_queue = queue.Queue(maxsize=100)
        PRE_BUFFER_SIZE = 3
        
        end_event = threading.Event()
        error_event = threading.Event()
        error_msg = ""
        
        def producer_thread():
            nonlocal error_msg
            try:
                previous_tail = None
                chunk_count = 0
                
                for i, chunk_text in enumerate(text_chunks):
                    stream_gen = tts.infer_stream(chunk_text, ref_codes, ref_text_raw)
                    
                    for part_idx, audio_part in enumerate(stream_gen):
                        if audio_part is None or len(audio_part) == 0:
                            continue
                        
                        if previous_tail is not None and len(previous_tail) > 0:
                            overlap = min(len(previous_tail), len(audio_part), crossfade_samples)
                            if overlap > 0:
                                fade_out = np.linspace(1.0, 0.0, overlap, dtype=np.float32)
                                fade_in = np.linspace(0.0, 1.0, overlap, dtype=np.float32)
                                
                                blended = (audio_part[:overlap] * fade_in + 
                                         previous_tail[-overlap:] * fade_out)
                                
                                processed = np.concatenate([
                                    previous_tail[:-overlap] if len(previous_tail) > overlap else np.array([]),
                                    blended,
                                    audio_part[overlap:]
                                ])
                            else:
                                processed = np.concatenate([previous_tail, audio_part])
                            
                            tail_size = min(crossfade_samples, len(processed))
                            previous_tail = processed[-tail_size:].copy()
                            output_chunk = processed[:-tail_size] if len(processed) > tail_size else processed
                        else:
                            tail_size = min(crossfade_samples, len(audio_part))
                            previous_tail = audio_part[-tail_size:].copy()
                            output_chunk = audio_part[:-tail_size] if len(audio_part) > tail_size else audio_part
                        
                        if len(output_chunk) > 0:
                            audio_queue.put((sr, output_chunk))
                            chunk_count += 1
                
                if previous_tail is not None and len(previous_tail) > 0:
                    audio_queue.put((sr, previous_tail))
                    
            except Exception as e:
                import traceback
                traceback.print_exc()
                error_msg = str(e)
                error_event.set()
            finally:
                end_event.set()
                audio_queue.put(None)
        
        threading.Thread(target=producer_thread, daemon=True).start()
        
        yield (sr, np.zeros(int(sr * 0.05))), "üîÑ ƒêang buffering..."
        
        pre_buffer = []
        while len(pre_buffer) < PRE_BUFFER_SIZE:
            try:
                item = audio_queue.get(timeout=5.0)
                if item is None:
                    break
                pre_buffer.append(item)
            except queue.Empty:
                if error_event.is_set():
                    yield None, f"‚ùå L·ªói: {error_msg}"
                    return
                break
        
        full_audio_buffer = []
        for sr, audio_data in pre_buffer:
            full_audio_buffer.append(audio_data)
            yield (sr, audio_data), "üîä ƒêang ph√°t..."
        
        while True:
            try:
                item = audio_queue.get(timeout=0.05)
                if item is None:
                    break
                sr, audio_data = item
                full_audio_buffer.append(audio_data)
                yield (sr, audio_data), "üîä ƒêang ph√°t..."
            except queue.Empty:
                if error_event.is_set():
                    yield None, f"‚ùå L·ªói: {error_msg}"
                    break
                if end_event.is_set() and audio_queue.empty():
                    break
                continue
        
        if full_audio_buffer:
            final_wav = np.concatenate(full_audio_buffer)
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                sf.write(tmp.name, final_wav, sr)
                yield tmp.name, "‚úÖ Ho√†n t·∫•t Streaming!"

# --- 4. UI SETUP ---
theme = gr.themes.Ocean(
    primary_hue="indigo",
    secondary_hue="cyan",
    neutral_hue="slate",
    font=[gr.themes.GoogleFont('Inter'), 'ui-sans-serif', 'system-ui'],
).set(
    button_primary_background_fill="linear-gradient(90deg, #6366f1 0%, #0ea5e9 100%)",
    button_primary_background_fill_hover="linear-gradient(90deg, #4f46e5 0%, #0284c7 100%)",
)

css = """
.container { max-width: 1400px; margin: auto; }
.header-box {
    text-align: center;
    margin-bottom: 25px;
    padding: 25px;
    background: linear-gradient(135deg, #0f172a 0%, #1e293b 100%);
    border-radius: 12px;
    color: white;
}
.header-title {
    font-size: 2.5rem;
    font-weight: 800;
    background: -webkit-linear-gradient(45deg, #60A5FA, #22D3EE);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
}
.status-box {
    font-weight: bold;
    text-align: center;
    border: none;
    background: transparent;
}
"""

EXAMPLES_LIST = [
    ["V·ªÅ mi·ªÅn T√¢y kh√¥ng ch·ªâ ƒë·ªÉ ng·∫Øm nh√¨n s√¥ng n∆∞·ªõc h·ªØu t√¨nh, m√† c√≤n ƒë·ªÉ c·∫£m nh·∫≠n t·∫•m ch√¢n t√¨nh c·ªßa ng∆∞·ªùi d√¢n n∆°i ƒë√¢y.", "Vƒ©nh (nam mi·ªÅn Nam)"],
    ["H√† N·ªôi nh·ªØng ng√†y v√†o thu mang m·ªôt v·∫ª ƒë·∫πp tr·∫ßm m·∫∑c v√† c·ªï k√≠nh ƒë·∫øn l·∫° th∆∞·ªùng.", "B√¨nh (nam mi·ªÅn B·∫Øc)"],
]

with gr.Blocks(theme=theme, css=css, title="VieNeu-TTS") as demo:
    with gr.Column(elem_classes="container"):
        gr.HTML("""
<div class="header-box">
    <h1 class="header-title">ü¶ú VieNeu-TTS Studio</h1>
</div>
        """)
        
        # --- CONFIGURATION ---
        with gr.Group():
            with gr.Row():
                backbone_select = gr.Dropdown(list(BACKBONE_CONFIGS.keys()), value="GGUF Q8", label="ü¶ú Backbone")
                codec_select = gr.Dropdown(list(CODEC_CONFIGS.keys()), value="NeuCodec (Standard)", label="üéµ Codec")
                device_choice = gr.Radio(["Auto", "CPU", "CUDA"], value="Auto", label="üñ•Ô∏è Device")
            
            
            btn_load = gr.Button("üîÑ T·∫£i Model", variant="primary")
            model_status = gr.Markdown("‚è≥ Ch∆∞a t·∫£i model.")
        
        with gr.Row(elem_classes="container"):
            # --- INPUT ---
            with gr.Column(scale=3):
                text_input = gr.Textbox(
                    label=f"VƒÉn b·∫£n (Streaming h·ªó tr·ª£ t·ªõi {MAX_TOTAL_CHARS_STREAMING} k√Ω t·ª±, chia chunk {MAX_CHARS_PER_CHUNK} k√Ω t·ª±)",
                    lines=4,
                    value="H√† N·ªôi, tr√°i tim c·ªßa Vi·ªát Nam, l√† m·ªôt th√†nh ph·ªë ng√†n nƒÉm vƒÉn hi·∫øn v·ªõi b·ªÅ d√†y l·ªãch s·ª≠ v√† vƒÉn h√≥a ƒë·ªôc ƒë√°o. B∆∞·ªõc ch√¢n tr√™n nh·ªØng con ph·ªë c·ªï k√≠nh quanh H·ªì Ho√†n Ki·∫øm, du kh√°ch nh∆∞ ƒë∆∞·ª£c du h√†nh ng∆∞·ª£c th·ªùi gian, chi√™m ng∆∞·ª°ng ki·∫øn tr√∫c Ph√°p c·ªï ƒëi·ªÉn h√≤a quy·ªán v·ªõi n√©t ki·∫øn tr√∫c truy·ªÅn th·ªëng Vi·ªát Nam. M·ªói con ph·ªë trong khu ph·ªë c·ªï mang m·ªôt t√™n g·ªçi ƒë·∫∑c tr∆∞ng, ph·∫£n √°nh ngh·ªÅ th·ªß c√¥ng truy·ªÅn th·ªëng t·ª´ng th·ªãnh h√†nh n∆°i ƒë√¢y nh∆∞ ph·ªë H√†ng B·∫°c, H√†ng ƒê√†o, H√†ng M√£. ·∫®m th·ª±c H√† N·ªôi c≈©ng l√† m·ªôt ƒëi·ªÉm nh·∫•n ƒë·∫∑c bi·ªát, t·ª´ t√¥ ph·ªü n√≥ng h·ªïi bu·ªïi s√°ng, b√∫n ch·∫£ th∆°m l·ª´ng tr∆∞a h√®, ƒë·∫øn ch√® Th√°i ng·ªçt ng√†o chi·ªÅu thu. Nh·ªØng m√≥n ƒÉn d√¢n d√£ n√†y ƒë√£ tr·ªü th√†nh bi·ªÉu t∆∞·ª£ng c·ªßa vƒÉn h√≥a ·∫©m th·ª±c Vi·ªát, ƒë∆∞·ª£c c·∫£ th·∫ø gi·ªõi y√™u m·∫øn. Ng∆∞·ªùi H√† N·ªôi n·ªïi ti·∫øng v·ªõi t√≠nh c√°ch hi·ªÅn h√≤a, l·ªãch thi·ªáp nh∆∞ng c≈©ng r·∫•t c·∫ßu to√†n trong t·ª´ng chi ti·∫øt nh·ªè, t·ª´ c√°ch pha tr√† sen cho ƒë·∫øn c√°ch ch·ªçn hoa sen t√¢y ƒë·ªÉ th∆∞·ªüng tr√†.",
                )
                
                with gr.Tabs() as tabs:
                    with gr.TabItem("üë§ Preset", id="preset_mode"):
                        initial_voices = get_voice_options("GGUF Q8")
                        default_voice = initial_voices[0] if initial_voices else None
                        voice_select = gr.Dropdown(initial_voices, value=default_voice, label="Gi·ªçng m·∫´u")
                    
                    with gr.TabItem("üéôÔ∏è Custom", id="custom_mode"):
                        custom_audio = gr.Audio(label="File m·∫´u (.wav)", type="filepath")
                        custom_text = gr.Textbox(label="L·ªùi tho·∫°i m·∫´u")
                
                generation_mode = gr.Radio(
                    ["Standard (M·ªôt l·∫ßn)"],
                    value="Standard (M·ªôt l·∫ßn)",
                    label="Ch·∫ø ƒë·ªô sinh"
                )
                
                current_mode = gr.Textbox(visible=False, value="preset_mode")
                
                # ‚ú® N√öT B·∫ÆT ƒê·∫¶U - M·∫∂C ƒê·ªäNH DISABLE
                btn_generate = gr.Button("üéµ B·∫Øt ƒë·∫ßu", variant="primary", size="lg", interactive=False)
            
            # --- OUTPUT ---
            with gr.Column(scale=2):
                audio_output = gr.Audio(
                    label="K·∫øt qu·∫£",
                    type="filepath",
                    autoplay=True,
                    show_download_button=True
                )
                status_output = gr.Textbox(label="Tr·∫°ng th√°i", elem_classes="status-box")
        
        # --- EVENT HANDLERS ---
        def update_info(backbone):
            return f"Streaming: {'‚úÖ' if BACKBONE_CONFIGS[backbone]['supports_streaming'] else '‚ùå'}"
        
        backbone_select.change(update_info, backbone_select, model_status)
        backbone_select.change(update_voice_dropdown, [backbone_select, voice_select], voice_select)
        
        tabs.children[0].select(lambda: "preset_mode", outputs=current_mode)
        tabs.children[1].select(lambda: "custom_mode", outputs=current_mode)
        
        # ‚ú® C·∫¨P NH·∫¨T EVENT HANDLER CHO N√öT LOAD
        btn_load.click(
            fn=load_model,
            inputs=[backbone_select, codec_select, device_choice],
            outputs=[model_status, btn_generate, btn_load]  # Update c·∫£ 3 components
        )
        
        btn_generate.click(
            fn=synthesize_speech,
            inputs=[text_input, voice_select, custom_audio, custom_text, current_mode, generation_mode],
            outputs=[audio_output, status_output]
        )

if __name__ == "__main__":
    demo.queue().launch(server_name="127.0.0.1", server_port=7860)